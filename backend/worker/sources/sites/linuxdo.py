import logging
import datetime
import hashlib
import xml.etree.ElementTree as ET
from typing import List, Dict, Any, Optional
import re
from urllib.parse import urlparse, parse_qs, unquote

from worker.sources.base import NewsItemModel
from worker.sources.rest_api import RESTNewsSource
from worker.utils.http_client import http_client

logger = logging.getLogger(__name__)


class LinuxDoNewsSource(RESTNewsSource):
    """
    Linuxè¿·æ–°é—»æºé€‚é…å™¨
    åŸå§‹ linux.do ç«™ç‚¹å¯èƒ½ä¼šè¿”å› 403 é”™è¯¯ï¼Œ
    æ­¤é€‚é…å™¨æ·»åŠ äº†å¤‡ç”¨æ¥æºï¼ŒåŒ…æ‹¬ LinuxCN å’Œ LinuxStoryï¼Œ
    ä»¥ç¡®ä¿å³ä½¿åŸå§‹ API ä¸å¯ç”¨ä¹Ÿèƒ½è·å–åˆ°ç›¸å…³å†…å®¹ã€‚
    """
    
    # å¤‡ç”¨æº RSS æº
    BACKUP_URLS = [
        "https://linux.cn/rss.xml",              # Linuxä¸­å›½ RSS æº
        "https://linuxstory.org/feed/",          # LinuxStory RSS æº
    ]
    
    def __init__(
        self,
        source_id: str = "linuxdo",
        name: str = "Linuxè¿·",
        api_url: str = "https://linux.do/latest.json?order=created",
        update_interval: int = 1800,  # 30åˆ†é’Ÿ
        cache_ttl: int = 900,  # 15åˆ†é’Ÿ
        category: str = "technology",
        country: str = "CN",
        language: str = "zh-CN",
        config: Optional[Dict[str, Any]] = None,
        mode: str = "latest"  # æ¨¡å¼ï¼šlatest æˆ– hot
    ):
        self.mode = mode
        
        # æ ¹æ®æ¨¡å¼è®¾ç½®API URL
        if mode == "hot":
            api_url = "https://linux.do/top/daily.json"
        
        config = config or {}
        config.update({
            "headers": {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36",
                "Accept": "application/json, application/xml, text/xml, application/rss+xml, */*",
                "Referer": "https://www.google.com/",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "DNT": "1",
                "Connection": "keep-alive"
            },
            "max_retries": 3,
            "retry_delay": 2,
            "timeout": 20,
            "use_backup_sources": True
        })
        
        self.backup_urls = config.get("backup_urls", self.BACKUP_URLS)
        
        super().__init__(
            source_id=source_id,
            name=name,
            api_url=api_url,
            update_interval=update_interval,
            cache_ttl=cache_ttl,
            category=category,
            country=country,
            language=language,
            custom_parser=self.custom_parser
        )
    
    async def fetch(self) -> List[NewsItemModel]:
        """
        è·å–Linuxç›¸å…³æ–°é—»ï¼Œæ·»åŠ é”™è¯¯å¤„ç†å’Œå¤‡ç”¨æº
        """
        logger.info(f"æ­£åœ¨ä»ä¸»è¦APIè·å–Linuxæ–°é—»: {self.api_url}")
        
        news_items = []
        try:
            # å°è¯•ä»åŸå§‹APIè·å–æ•°æ®
            response = await http_client.fetch(
                url=self.api_url,
                method="GET",
                params=self.params,
                headers=self.headers,
                response_type="json",
                timeout=self.config.get("timeout", 15)
            )
            
            # ä½¿ç”¨è§£æå™¨å¤„ç†å“åº”
            news_items = self.custom_parser(response)
            logger.info(f"ä»åŸå§‹APIè·å–åˆ° {len(news_items)} æ¡æ–°é—»")
            
        except Exception as e:
            logger.warning(f"åŸå§‹APIè¯·æ±‚å¤±è´¥: {str(e)}")
            
            # å¦‚æœå¯ç”¨äº†å¤‡ç”¨æºé€‰é¡¹ï¼Œå°è¯•ä»å¤‡ç”¨æºè·å–æ•°æ®
            if self.config.get("use_backup_sources", True) and self.backup_urls:
                logger.info("å°è¯•ä½¿ç”¨å¤‡ç”¨æºè·å–æ•°æ®...")
                backup_items = await self._fetch_from_backup_sources()
                
                if backup_items:
                    logger.info(f"ä»å¤‡ç”¨æºè·å–åˆ° {len(backup_items)} æ¡æ–°é—»")
                    news_items = backup_items
                else:
                    logger.warning("å¤‡ç”¨æºä¹Ÿæœªèƒ½è·å–æ•°æ®")
                    # ä¸å†åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ï¼Œè€Œæ˜¯æŠ›å‡ºå¼‚å¸¸
                    raise RuntimeError("æ— æ³•è·å–Linuxæ–°é—»æ•°æ®ï¼šåŸå§‹APIå’Œæ‰€æœ‰å¤‡ç”¨æºå‡å¤±è´¥")
            else:
                # ä¸å†åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ï¼Œè€Œæ˜¯æŠ›å‡ºå¼‚å¸¸
                logger.error("åŸå§‹APIå¤±è´¥ä¸”æœªå¯ç”¨å¤‡ç”¨æº")
                raise RuntimeError(f"æ— æ³•è·å–Linuxæ–°é—»æ•°æ®ï¼šAPIè¯·æ±‚å¤±è´¥ - {str(e)}")
        
        return news_items
    
    async def _fetch_from_backup_sources(self) -> List[NewsItemModel]:
        """
        ä»å¤‡ç”¨RSSæºè·å–æ•°æ®
        """
        news_items = []
        
        for url in self.backup_urls:
            try:
                logger.info(f"å°è¯•ä»å¤‡ç”¨æºè·å–æ•°æ®: {url}")
                
                response = await http_client.fetch(
                    url=url,
                    method="GET",
                    headers=self.headers,
                    response_type="text",
                    timeout=self.config.get("timeout", 15)
                )
                
                # è§£æRSSå“åº”
                items = await self._parse_rss(response, url)
                
                if items:
                    news_items.extend(items)
                    logger.info(f"ä» {url} è·å–åˆ° {len(items)} æ¡æ–°é—»")
                    
                    # å¦‚æœè·å–è¶³å¤Ÿçš„æ¡ç›®ï¼Œå°±åœæ­¢
                    if len(news_items) >= 30:
                        break
            except Exception as e:
                logger.warning(f"ä»å¤‡ç”¨æº {url} è·å–æ•°æ®å¤±è´¥: {str(e)}")
        
        # æ ¹æ®æ¨¡å¼æ’åº
        if self.mode == "latest":
            # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            news_items.sort(key=lambda x: x.published_at if x.published_at else datetime.datetime.now(datetime.timezone.utc), reverse=True)
        else:
            # çƒ­é—¨æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨éšæœºé¡ºåº
            import random
            random.shuffle(news_items)
        
        return news_items[:50]  # é™åˆ¶è¿”å›çš„æ¡ç›®æ•°
    
    async def _parse_rss(self, content: str, source_url: str) -> List[NewsItemModel]:
        """
        è§£æRSSå†…å®¹å¹¶åˆ›å»ºæ–°é—»é¡¹
        """
        news_items = []
        source_domain = urlparse(source_url).netloc
        
        try:
            # å¤„ç†å¯èƒ½çš„XMLå£°æ˜é—®é¢˜
            if not content.strip().startswith('<?xml'):
                content = f'<?xml version="1.0" encoding="UTF-8"?>{content}'
            
            # è§£æXML
            root = ET.fromstring(content)
            
            # æŸ¥æ‰¾RSSä¸­çš„itemå…ƒç´ 
            channel = root.find('channel')
            if channel is None:
                return []
                
            items = channel.findall('item')
            
            for item in items:
                try:
                    # è·å–æ ‡é¢˜
                    title_elem = item.find('title')
                    if title_elem is None or not title_elem.text:
                        continue
                    
                    # å¤„ç†CDATAåŒ…è£…çš„æ ‡é¢˜
                    title = title_elem.text
                    if title.startswith('<![CDATA[') and title.endswith(']]>'):
                        title = title[9:-3]
                    
                    # è·å–é“¾æ¥
                    link_elem = item.find('link')
                    if link_elem is None or not link_elem.text:
                        continue
                    url = link_elem.text
                    
                    # ç”Ÿæˆå”¯ä¸€ID
                    item_id = hashlib.md5(f"{self.source_id}:{url}".encode()).hexdigest()
                    
                    # è·å–å‘å¸ƒæ—¶é—´
                    published_at = None
                    pub_date_elem = item.find('pubDate')
                    if pub_date_elem is not None and pub_date_elem.text:
                        try:
                            from email.utils import parsedate_to_datetime
                            published_at = parsedate_to_datetime(pub_date_elem.text)
                        except:
                            pass
                    
                    # è·å–æè¿°/æ‘˜è¦
                    summary = None
                    desc_elem = item.find('description')
                    if desc_elem is not None and desc_elem.text:
                        # æå–çº¯æ–‡æœ¬ï¼Œç§»é™¤HTMLæ ‡ç­¾
                        summary_text = desc_elem.text
                        if summary_text.startswith('<![CDATA[') and summary_text.endswith(']]>'):
                            summary_text = summary_text[9:-3]
                        
                        summary = re.sub(r'<[^>]+>', ' ', summary_text)
                        summary = re.sub(r'\s+', ' ', summary).strip()
                    
                    # è·å–ä½œè€…
                    author = None
                    creator_elem = item.find('.//{http://purl.org/dc/elements/1.1/}creator')
                    if creator_elem is not None and creator_elem.text:
                        author_text = creator_elem.text
                        if author_text.startswith('<![CDATA[') and author_text.endswith(']]>'):
                            author = author_text[9:-3]
                        else:
                            author = author_text
                    
                    # è·å–åˆ†ç±»
                    categories = []
                    for cat_elem in item.findall('category'):
                        if cat_elem.text:
                            cat_text = cat_elem.text
                            if cat_text.startswith('<![CDATA[') and cat_text.endswith(']]>'):
                                categories.append(cat_text[9:-3])
                            else:
                                categories.append(cat_text)
                    
                    # åˆ›å»ºæ–°é—»é¡¹
                    news_item = self.create_news_item(
                        id=item_id,
                        title=title,
                        url=url,
                        content=None,
                        summary=summary,
                        image_url=None,
                        published_at=published_at,
                        extra={
                            "is_top": False,
                            "mobile_url": url,
                            "author": author,
                            "categories": categories,
                            "source_from": f"rss:{source_domain}"
                        }
                    )
                    
                    news_items.append(news_item)
                except Exception as e:
                    logger.error(f"å¤„ç†RSSæ¡ç›®æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            return news_items
        
        except Exception as e:
            logger.error(f"è§£æRSSå†…å®¹æ—¶å‡ºé”™: {str(e)}")
            return []
    
    def custom_parser(self, data: Dict[str, Any]) -> List[NewsItemModel]:
        """
        è‡ªå®šä¹‰è§£æå™¨ï¼Œå¤„ç†Linuxä¸­å›½çš„JSONæ•°æ®
        """
        news_items = []
        
        try:
            # è·å–ä¸»é¢˜åˆ—è¡¨
            topic_list = data.get("topic_list", {})
            topics = topic_list.get("topics", [])
            
            for topic in topics:
                try:
                    # æ£€æŸ¥ä¸»é¢˜æ˜¯å¦å¯è§ã€æœªå½’æ¡£ã€æœªç½®é¡¶
                    visible = topic.get("visible", False)
                    archived = topic.get("archived", False)
                    pinned = topic.get("pinned", False)
                    
                    if not visible or archived:
                        continue
                    
                    # è·å–ä¸»é¢˜ID
                    topic_id = topic.get("id")
                    if not topic_id:
                        continue
                    
                    # è·å–æ ‡é¢˜
                    title = topic.get("title")
                    if not title:
                        continue
                    
                    # ç”ŸæˆURL
                    url = f"https://linux.do/t/topic/{topic_id}"
                    
                    # è·å–åˆ›å»ºæ—¶é—´
                    created_at_str = topic.get("created_at")
                    published_at = None
                    if created_at_str:
                        try:
                            published_at = datetime.datetime.fromisoformat(created_at_str.replace("Z", "+00:00"))
                        except Exception as e:
                            logger.error(f"Error parsing date {created_at_str}: {str(e)}")
                    
                    # è·å–æ‘˜è¦
                    excerpt = topic.get("excerpt")
                    
                    # è·å–å›å¤æ•°å’Œç‚¹èµæ•°
                    reply_count = topic.get("reply_count", 0)
                    like_count = topic.get("like_count", 0)
                    
                    # åˆ›å»ºæ–°é—»é¡¹
                    news_item = self.create_news_item(
                        id=str(topic_id),
                        title=title,
                        url=url,
                        content=None,
                        summary=excerpt,
                        image_url=None,
                        published_at=published_at,
                        extra={
                            "is_top": pinned,
                            "mobile_url": url,
                            "like_count": like_count,
                            "comment_count": reply_count,
                            "info": f"ğŸ‘ {like_count} ğŸ’¬ {reply_count}",
                            "source_from": "linux.do"
                        }
                    )
                    
                    news_items.append(news_item)
                except Exception as e:
                    logger.error(f"Error processing LinuxDo topic: {str(e)}")
                    continue
            
            # å¦‚æœæ˜¯æœ€æ–°æ¨¡å¼ï¼ŒæŒ‰åˆ›å»ºæ—¶é—´æ’åº
            if self.mode == "latest":
                news_items.sort(key=lambda x: x.published_at if x.published_at else datetime.datetime.now(datetime.timezone.utc), reverse=True)
            
            return news_items
        except Exception as e:
            logger.error(f"Error parsing LinuxDo response: {str(e)}")
            return []
    
    def _create_mock_data(self) -> List[NewsItemModel]:
        """
        åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ä½œä¸ºæœ€åçš„å¤‡ç”¨é€‰é¡¹
        """
        logger.info("åˆ›å»ºLinuxç›¸å…³æ¨¡æ‹Ÿæ•°æ®")
        
        # æ¨¡æ‹Ÿæ ‡é¢˜å’ŒURLæ¨¡æ¿
        mock_items = [
            {
                "title": "å¦‚ä½•åœ¨Linuxä¸­ä½¿ç”¨Dockerå®¹å™¨åŒ–åº”ç”¨ç¨‹åº",
                "url": "https://linux.cn/article-mock-1.html",
                "summary": "æœ¬æ•™ç¨‹è¯¦ç»†ä»‹ç»äº†Dockeråœ¨Linuxä¸­çš„å®‰è£…å’ŒåŸºæœ¬ä½¿ç”¨æ–¹æ³•ï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€Ÿä¸Šæ‰‹å®¹å™¨åŒ–æŠ€æœ¯ã€‚"
            },
            {
                "title": "Linux 6.0å†…æ ¸å‘å¸ƒï¼Œå¸¦æ¥é‡å¤§æ€§èƒ½æå‡",
                "url": "https://linux.cn/article-mock-2.html",
                "summary": "æœ€æ–°çš„Linux 6.0å†…æ ¸å·²å‘å¸ƒï¼Œæ”¹è¿›äº†è°ƒåº¦å™¨æ€§èƒ½ï¼Œå¢å¼ºäº†å®‰å…¨ç‰¹æ€§ï¼Œå¹¶æä¾›äº†æ›´å¥½çš„ç¡¬ä»¶æ”¯æŒã€‚"
            },
            {
                "title": "å¼€æºäººå·¥æ™ºèƒ½å·¥å…·åœ¨Linuxç¯å¢ƒä¸‹çš„éƒ¨ç½²æŒ‡å—",
                "url": "https://linux.cn/article-mock-3.html",
                "summary": "æœ¬æ–‡ä»‹ç»å¦‚ä½•åœ¨Linuxç³»ç»Ÿä¸­éƒ¨ç½²å’Œä½¿ç”¨æµè¡Œçš„å¼€æºAIå·¥å…·ï¼ŒåŒ…æ‹¬TensorFlowå’ŒPyTorchæ¡†æ¶ã€‚"
            },
            {
                "title": "LinuxæœåŠ¡å™¨å®‰å…¨åŠ å›ºæœ€ä½³å®è·µ",
                "url": "https://linux.cn/article-mock-4.html",
                "summary": "ä¿æŠ¤ä½ çš„LinuxæœåŠ¡å™¨å…å—æ”»å‡»çš„å®ç”¨æŠ€å·§ï¼Œä»åŸºæœ¬çš„ç”¨æˆ·æƒé™ç®¡ç†åˆ°é«˜çº§çš„å…¥ä¾µæ£€æµ‹ç³»ç»Ÿã€‚"
            },
            {
                "title": "äº”ä¸ªæé«˜Linuxç»ˆç«¯æ•ˆç‡çš„éšè—æŠ€å·§",
                "url": "https://linux.cn/article-mock-5.html",
                "summary": "è¿™äº›é²œä¸ºäººçŸ¥çš„ç»ˆç«¯ä½¿ç”¨æŠ€å·§å°†æ˜¾è‘—æå‡ä½ çš„å‘½ä»¤è¡Œå·¥ä½œæ•ˆç‡ã€‚"
            },
            {
                "title": "å¦‚ä½•é…ç½®Linuxç³»ç»Ÿå®ç°æœ€ä½³æ€§èƒ½",
                "url": "https://linux.cn/article-mock-6.html",
                "summary": "ä»å†…æ ¸å‚æ•°ä¼˜åŒ–åˆ°æ–‡ä»¶ç³»ç»Ÿé€‰æ‹©ï¼Œæœ¬æ–‡å…¨é¢ä»‹ç»å¦‚ä½•è°ƒæ•´Linuxç³»ç»Ÿä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚"
            },
            {
                "title": "åœ¨Linuxä¸‹æ­å»ºé«˜æ€§èƒ½WebæœåŠ¡å™¨çš„å®Œæ•´æŒ‡å—",
                "url": "https://linux.cn/article-mock-7.html",
                "summary": "ä½¿ç”¨Nginxå’Œä¼˜åŒ–çš„Linuxé…ç½®ï¼Œæ„å»ºèƒ½å¤Ÿæ‰¿è½½é«˜æµé‡çš„ç°ä»£WebæœåŠ¡å™¨ã€‚"
            },
            {
                "title": "Linuxå‘è¡Œç‰ˆæ¯”è¾ƒï¼šUbuntuã€Fedoraå’ŒArchçš„ä¼˜ç¼ºç‚¹åˆ†æ",
                "url": "https://linux.cn/article-mock-8.html",
                "summary": "è¯¦ç»†å¯¹æ¯”ä¸‰ç§æµè¡Œçš„Linuxå‘è¡Œç‰ˆï¼Œå¸®åŠ©ç”¨æˆ·æ ¹æ®è‡ªå·±çš„éœ€æ±‚é€‰æ‹©æœ€åˆé€‚çš„ç³»ç»Ÿã€‚"
            },
            {
                "title": "å¦‚ä½•åœ¨Linuxä¸­å®ç°è‡ªåŠ¨åŒ–ç³»ç»Ÿç®¡ç†",
                "url": "https://linux.cn/article-mock-9.html",
                "summary": "ä½¿ç”¨Ansibleã€Shellè„šæœ¬å’ŒCronä½œä¸šï¼Œæ„å»ºå®Œå…¨è‡ªåŠ¨åŒ–çš„Linuxç³»ç»Ÿç®¡ç†æ–¹æ¡ˆã€‚"
            },
            {
                "title": "Linuxä¸‹çš„ç°ä»£å¼€å‘ç¯å¢ƒæ­å»º",
                "url": "https://linux.cn/article-mock-10.html",
                "summary": "ä»ä»£ç ç¼–è¾‘å™¨åˆ°ç‰ˆæœ¬æ§åˆ¶ï¼Œä»å®¹å™¨åŒ–å·¥å…·åˆ°CI/CDç®¡é“ï¼Œæ‰“é€ å®Œç¾çš„Linuxå¼€å‘ç¯å¢ƒã€‚"
            }
        ]
        
        now = datetime.datetime.now(datetime.timezone.utc)
        news_items = []
        
        for i, item in enumerate(mock_items):
            # åˆ›å»ºéšæœºå‘å¸ƒæ—¶é—´ï¼ˆæœ€è¿‘7å¤©å†…ï¼‰
            hours_ago = i * 5 + i % 3  # åˆ›å»ºä¸€äº›æ—¶é—´å·®å¼‚
            published_at = now - datetime.timedelta(hours=hours_ago)
            
            # ç”ŸæˆID
            item_id = hashlib.md5(f"mock:{item['url']}".encode()).hexdigest()
            
            # åˆ›å»ºæ–°é—»é¡¹
            news_item = self.create_news_item(
                id=item_id,
                title=item["title"],
                url=item["url"],
                content=None,
                summary=item["summary"],
                image_url=None,
                published_at=published_at,
                extra={
                    "is_top": i < 3,  # å‰ä¸‰ä¸ªæ˜¯ç½®é¡¶
                    "mobile_url": item["url"],
                    "like_count": 10 + i * 5,
                    "comment_count": 5 + i * 2,
                    "info": f"ğŸ‘ {10 + i * 5} ğŸ’¬ {5 + i * 2}",
                    "source_from": "mock_data",
                    "is_mock": True
                }
            )
            
            news_items.append(news_item)
        
        return news_items


class LinuxDoLatestNewsSource(LinuxDoNewsSource):
    """
    Linuxè¿·æœ€æ–°ä¸»é¢˜é€‚é…å™¨
    """
    
    def __init__(
        self,
        source_id: str = "linuxdo-latest",
        name: str = "Linuxè¿·æœ€æ–°",
        **kwargs
    ):
        super().__init__(
            source_id=source_id,
            name=name,
            mode="latest",
            **kwargs
        )


class LinuxDoHotNewsSource(LinuxDoNewsSource):
    """
    Linuxè¿·çƒ­é—¨ä¸»é¢˜é€‚é…å™¨
    """
    
    def __init__(
        self,
        source_id: str = "linuxdo-hot",
        name: str = "Linuxè¿·çƒ­é—¨",
        **kwargs
    ):
        super().__init__(
            source_id=source_id,
            name=name,
            mode="hot",
            **kwargs
        ) 