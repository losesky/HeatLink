#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¼“å­˜ç›‘æ§å·¥å…·

ç›‘æ§å¹¶åˆ†æå„ä¸ªæ–°é—»æºçš„ç¼“å­˜è¡Œä¸ºï¼Œç”ŸæˆæŠ¥å‘Šï¼Œå¸®åŠ©ä¼˜åŒ–ç¼“å­˜ç­–ç•¥
"""

import os
import sys
import asyncio
import logging
import time
import json
import datetime
from typing import List, Dict, Any, Optional, Tuple
import argparse
import sqlite3
import pandas as pd
import matplotlib.pyplot as plt
from tabulate import tabulate

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("cache_monitor")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°PATH
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

# æ­£ç¡®çš„å¯¼å…¥
from worker.sources.provider import DefaultNewsSourceProvider
from worker.sources.base import NewsSource
from backend.worker.utils.cache_enhancer import enhance_all_sources, cache_monitor

# åˆ›å»ºç¼“å­˜ç›‘æ§å¤„ç†å™¨
class CacheMonitor:
    """ç¼“å­˜è¡Œä¸ºç›‘æ§å·¥å…·"""
    
    def __init__(self, db_path="cache_monitor.db"):
        """åˆå§‹åŒ–ç›‘æ§å™¨"""
        self.db_path = db_path
        self.conn = None
        self.setup_database()
        
    def setup_database(self):
        """è®¾ç½®æ•°æ®åº“"""
        try:
            self.conn = sqlite3.connect(self.db_path)
            cursor = self.conn.cursor()
            
            # åˆ›å»ºæºé…ç½®è¡¨
            cursor.execute('''
            CREATE TABLE IF NOT EXISTS sources (
                source_id TEXT PRIMARY KEY,
                name TEXT,
                type TEXT,
                cache_ttl INTEGER,
                update_interval INTEGER,
                last_check TIMESTAMP
            )
            ''')
            
            # åˆ›å»ºç¼“å­˜äº‹ä»¶è¡¨
            cursor.execute('''
            CREATE TABLE IF NOT EXISTS cache_events (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_id TEXT,
                event_type TEXT,
                event_time TIMESTAMP,
                cache_size INTEGER,
                cache_age REAL,
                response_time REAL,
                success INTEGER,
                message TEXT,
                FOREIGN KEY (source_id) REFERENCES sources (source_id)
            )
            ''')
            
            # åˆ›å»ºæ€§èƒ½æŒ‡æ ‡è¡¨
            cursor.execute('''
            CREATE TABLE IF NOT EXISTS performance (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_id TEXT,
                timestamp TIMESTAMP,
                force_update INTEGER,
                response_time REAL,
                news_count INTEGER,
                from_cache INTEGER,
                cache_valid INTEGER,
                FOREIGN KEY (source_id) REFERENCES sources (source_id)
            )
            ''')
            
            self.conn.commit()
            logger.info(f"æ•°æ®åº“è®¾ç½®å®Œæˆ: {self.db_path}")
        except Exception as e:
            logger.error(f"è®¾ç½®æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
            if self.conn:
                self.conn.close()
            raise
            
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            
    async def register_sources(self):
        """ä»æºæä¾›è€…æ³¨å†Œæ‰€æœ‰å¯ç”¨æº"""
        try:
            provider = DefaultNewsSourceProvider()
            all_sources = provider.get_all_sources()
            
            # æ„å»ºä¸åŸä»£ç å…¼å®¹çš„å­—å…¸æ ¼å¼
            sources = {}
            for source in all_sources:
                sources[source.source_id] = {
                    'name': getattr(source, 'name', source.source_id),
                    'category': getattr(source, 'category', ''),
                    'country': getattr(source, 'country', ''),
                    'language': getattr(source, 'language', '')
                }
            
            cursor = self.conn.cursor()
            for source_id, info in sources.items():
                # è·å–æºå®ä¾‹æ¥è·å–è¯¦ç»†é…ç½®
                source = provider.get_source(source_id)
                if not source:
                    continue
                    
                source_type = source.__class__.__name__
                cache_ttl = getattr(source, 'cache_ttl', 0)
                update_interval = getattr(source, 'update_interval', 0)
                
                # æ’å…¥æˆ–æ›´æ–°æºä¿¡æ¯
                cursor.execute('''
                INSERT OR REPLACE INTO sources 
                (source_id, name, type, cache_ttl, update_interval, last_check) 
                VALUES (?, ?, ?, ?, ?, ?)
                ''', (source_id, info.get('name', ''), source_type, 
                     cache_ttl, update_interval, datetime.datetime.now().isoformat()))
            
            self.conn.commit()
            logger.info(f"å·²æ³¨å†Œ {len(sources)} ä¸ªæ–°é—»æº")
            return sources
        except Exception as e:
            logger.error(f"æ³¨å†Œæºæ—¶å‡ºé”™: {str(e)}")
            return {}
    
    async def test_source_caching(self, source_id: str, test_count: int = 3):
        """æµ‹è¯•æŒ‡å®šæºçš„ç¼“å­˜è¡Œä¸º"""
        logging.info(f"å¼€å§‹æµ‹è¯• {source_id} æºçš„ç¼“å­˜è¡Œä¸º...")
        
        provider = DefaultNewsSourceProvider()
        
        # è·å–æºå¹¶åº”ç”¨ç¼“å­˜å¢å¼º
        provider_sources = provider.get_all_sources()
        
        # æŸ¥æ‰¾æŒ‡å®šçš„æº
        source = None
        if isinstance(provider_sources, dict):
            if source_id not in provider_sources:
                logging.error(f"æœªæ‰¾åˆ°æº {source_id}")
                return
            source = provider_sources[source_id]
        else:
            # å¦‚æœprovider_sourcesæ˜¯åˆ—è¡¨ï¼Œéå†æŸ¥æ‰¾åŒ¹é…çš„source_id
            source = next((s for s in provider_sources if s.source_id == source_id), None)
            if not source:
                logging.error(f"æœªæ‰¾åˆ°æº {source_id}")
                return
        
        # æ³¨å†Œåˆ°ç¼“å­˜ç›‘æ§å™¨ - è¿™ä¼šè‡ªåŠ¨å¢å¼ºæº
        cache_monitor.register_source(source)
        
        logging.info(f"æºåˆå§‹åŒ–å®Œæˆ: {source.source_id} - {source.name}")
        logging.info(f"ç¼“å­˜è®¾ç½®: update_interval={source.update_interval}, cache_ttl={source.cache_ttl}")
        
        # æµ‹è¯•åœºæ™¯1: å¼ºåˆ¶æ›´æ–° (ç»•è¿‡ç¼“å­˜ï¼Œæµ‹è¯•å®é™…è·å–æ€§èƒ½)
        logging.info("\n=== åœºæ™¯1: å¼ºåˆ¶æ›´æ–° ===")
        forced_times = []
        for i in range(test_count):
            start_time = time.time()
            news = await source.get_news(force_update=True)
            elapsed = time.time() - start_time
            forced_times.append(elapsed)
            
            logging.info(f"å¼ºåˆ¶æ›´æ–° #{i+1}: è·å–äº† {len(news)} æ¡æ–°é—», è€—æ—¶: {elapsed:.3f}ç§’")
            if i < test_count - 1:
                await asyncio.sleep(1)  # çŸ­æš‚æš‚åœé˜²æ­¢è¯·æ±‚é¢‘ç‡è¿‡é«˜
        
        avg_forced = sum(forced_times) / len(forced_times)
        logging.info(f"å¼ºåˆ¶æ›´æ–°å¹³å‡è€—æ—¶: {avg_forced:.3f}ç§’")
        
        # æµ‹è¯•åœºæ™¯2: ç¼“å­˜æ£€ç´¢ (ä¸å¼ºåˆ¶æ›´æ–°ï¼Œä½¿ç”¨ç¼“å­˜)
        logging.info("\n=== åœºæ™¯2: ç¼“å­˜æ£€ç´¢ ===")
        cache_times = []
        for i in range(test_count):
            start_time = time.time()
            news = await source.get_news(force_update=False)
            elapsed = time.time() - start_time
            cache_times.append(elapsed)
            
            logging.info(f"ç¼“å­˜æ£€ç´¢ #{i+1}: è·å–äº† {len(news)} æ¡æ–°é—», è€—æ—¶: {elapsed:.3f}ç§’")
            if i < test_count - 1:
                await asyncio.sleep(0.5)
        
        avg_cache = sum(cache_times) / len(cache_times) if cache_times else 0
        logging.info(f"ç¼“å­˜æ£€ç´¢å¹³å‡è€—æ—¶: {avg_cache:.3f}ç§’")
        
        # æµ‹è¯•åœºæ™¯3: ç¼“å­˜æ¸…é™¤åæ£€ç´¢ (æ¨¡æ‹Ÿç¼“å­˜è¿‡æœŸ)
        logging.info("\n=== åœºæ™¯3: ç¼“å­˜æ¸…é™¤ ===")
        await source.clear_cache()
        start_time = time.time()
        news = await source.get_news()
        elapsed = time.time() - start_time
        
        logging.info(f"æ¸…é™¤ç¼“å­˜åæ£€ç´¢: è·å–äº† {len(news)} æ¡æ–°é—», è€—æ—¶: {elapsed:.3f}ç§’")
        
        # æ±‡æ€»å½“å‰ç¼“å­˜æŒ‡æ ‡å’Œä¿æŠ¤çŠ¶æ€
        cache_status = source.cache_status()
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        acceleration_ratio = avg_forced / max(avg_cache, 0.001)
        
        logging.info("\n=== æµ‹è¯•ç»“æœæ±‡æ€» ===")
        logging.info(f"å¼ºåˆ¶æ›´æ–°å¹³å‡è€—æ—¶: {avg_forced:.3f}ç§’")
        logging.info(f"ç¼“å­˜æ£€ç´¢å¹³å‡è€—æ—¶: {avg_cache:.3f}ç§’")
        logging.info(f"ç¼“å­˜åŠ é€Ÿæ¯”: {acceleration_ratio:.2f}x")
        
        # è¾“å‡ºç¼“å­˜çŠ¶æ€å’Œä¿æŠ¤æŒ‡æ ‡
        logging.info("\n=== ç¼“å­˜çŠ¶æ€å’Œä¿æŠ¤æŒ‡æ ‡ ===")
        logging.info(f"ç¼“å­˜æ¡ç›®æ•°: {cache_status['cache_state']['items_count']}")
        logging.info(f"ç¼“å­˜æœ‰æ•ˆæ€§: {cache_status['cache_state']['valid']}")
        logging.info(f"ç¼“å­˜å‘½ä¸­ç‡: {cache_status['metrics']['hit_ratio']:.2%}")
        
        # ä¿æŠ¤ç»Ÿè®¡
        protections = cache_status['protection_stats']
        logging.info(f"ä¿æŠ¤ç»Ÿè®¡: ç©ºç»“æœä¿æŠ¤={protections['empty_protection_count']}, " +
                     f"é”™è¯¯ä¿æŠ¤={protections['error_protection_count']}, " +
                     f"æ•°æ®å‡å°‘ä¿æŠ¤={protections['shrink_protection_count']}")
        
        if protections['total_protection_count'] > 0:
            logging.info("ä¿æŠ¤æœºåˆ¶å·²æ¿€æ´»ï¼Œç¼“å­˜ä¿æŒäº†æ•°æ®å®Œæ•´æ€§")
    
    def _record_performance(self, source_id: str, force_update: bool, 
                          response_time: float, news_count: int,
                          from_cache: bool, cache_valid: bool):
        """è®°å½•æ€§èƒ½æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
            INSERT INTO performance 
            (source_id, timestamp, force_update, response_time, news_count, from_cache, cache_valid) 
            VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (source_id, datetime.datetime.now().isoformat(), 
                 1 if force_update else 0, response_time, news_count,
                 1 if from_cache else 0, 1 if cache_valid else 0))
            self.conn.commit()
        except Exception as e:
            logger.error(f"è®°å½•æ€§èƒ½æ•°æ®æ—¶å‡ºé”™: {str(e)}")
    
    def record_cache_event(self, source_id: str, event_type: str, cache_size: int,
                         cache_age: float, response_time: float, success: bool, message: str = ""):
        """è®°å½•ç¼“å­˜äº‹ä»¶"""
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
            INSERT INTO cache_events 
            (source_id, event_type, event_time, cache_size, cache_age, response_time, success, message) 
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (source_id, event_type, datetime.datetime.now().isoformat(), 
                 cache_size, cache_age, response_time, 1 if success else 0, message))
            self.conn.commit()
        except Exception as e:
            logger.error(f"è®°å½•ç¼“å­˜äº‹ä»¶æ—¶å‡ºé”™: {str(e)}")
    
    def analyze_performance(self, source_id: Optional[str] = None, days: int = 7):
        """åˆ†æç¼“å­˜æ€§èƒ½"""
        try:
            query = '''
            SELECT source_id, force_update, AVG(response_time) as avg_time,
                   COUNT(*) as count, AVG(news_count) as avg_news_count,
                   SUM(from_cache) as cache_hits
            FROM performance 
            WHERE timestamp > datetime('now', '-{} days')
            '''.format(days)
            
            if source_id:
                query += f" AND source_id = '{source_id}'"
                
            query += " GROUP BY source_id, force_update"
            
            df = pd.read_sql_query(query, self.conn)
            
            # è®¡ç®—ç¼“å­˜åŠ é€Ÿæ¯”
            results = []
            
            for sid in df['source_id'].unique():
                force_df = df[(df['source_id'] == sid) & (df['force_update'] == 1)]
                cache_df = df[(df['source_id'] == sid) & (df['force_update'] == 0)]
                
                if not force_df.empty and not cache_df.empty:
                    force_time = force_df['avg_time'].values[0]
                    cache_time = cache_df['avg_time'].values[0]
                    speedup = force_time / cache_time if cache_time > 0 else 0
                    
                    results.append({
                        'source_id': sid,
                        'force_update_time': force_time,
                        'cache_time': cache_time,
                        'speedup': speedup,
                        'force_count': force_df['count'].values[0],
                        'cache_count': cache_df['count'].values[0],
                        'cache_hits': cache_df['cache_hits'].values[0],
                        'cache_hit_rate': cache_df['cache_hits'].values[0] / cache_df['count'].values[0] if cache_df['count'].values[0] > 0 else 0,
                        'avg_news_count': cache_df['avg_news_count'].values[0]
                    })
            
            return results
        except Exception as e:
            logger.error(f"åˆ†ææ€§èƒ½æ—¶å‡ºé”™: {str(e)}")
            return []
    
    def generate_report(self, days: int = 7):
        """ç”Ÿæˆç¼“å­˜æ€§èƒ½æŠ¥å‘Š"""
        try:
            # è·å–æºä¿¡æ¯
            cursor = self.conn.cursor()
            cursor.execute('SELECT source_id, name, type, cache_ttl, update_interval FROM sources')
            sources = {row[0]: {'name': row[1], 'type': row[2], 'cache_ttl': row[3], 'update_interval': row[4]} 
                      for row in cursor.fetchall()}
            
            # åˆ†ææ€§èƒ½
            performance = self.analyze_performance(days=days)
            
            # ç”ŸæˆæŠ¥å‘Š
            report = {
                'timestamp': datetime.datetime.now().isoformat(),
                'period_days': days,
                'sources': len(sources),
                'analyzed_sources': len(performance),
                'summary': {
                    'avg_speedup': sum(p['speedup'] for p in performance) / len(performance) if performance else 0,
                    'effective_cache_count': sum(1 for p in performance if p['speedup'] > 1.5),
                    'avg_cache_hit_rate': sum(p['cache_hit_rate'] for p in performance) / len(performance) if performance else 0
                },
                'source_details': []
            }
            
            # æ·»åŠ æ¯ä¸ªæºçš„è¯¦ç»†ä¿¡æ¯
            for p in performance:
                source_id = p['source_id']
                source_info = sources.get(source_id, {})
                
                report['source_details'].append({
                    'source_id': source_id,
                    'name': source_info.get('name', ''),
                    'type': source_info.get('type', ''),
                    'cache_ttl': source_info.get('cache_ttl', 0),
                    'update_interval': source_info.get('update_interval', 0),
                    'performance': {
                        'force_update_time': p['force_update_time'],
                        'cache_time': p['cache_time'],
                        'speedup': p['speedup'],
                        'cache_effective': p['speedup'] > 1.5,
                        'cache_hit_rate': p['cache_hit_rate'],
                        'avg_news_count': p['avg_news_count']
                    }
                })
            
            return report
        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")
            return {}
    
    def save_report(self, report, output_file="cache_report.json"):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")
    
    def print_report_summary(self, report):
        """æ‰“å°æŠ¥å‘Šæ‘˜è¦"""
        if not report:
            logger.error("æ— æŠ¥å‘Šæ•°æ®")
            return
            
        print("\n" + "="*80)
        print(f"ç¼“å­˜æ€§èƒ½æŠ¥å‘Šæ‘˜è¦ ({report['timestamp']})")
        print("="*80)
        
        print(f"\nåˆ†æå‘¨æœŸ: {report['period_days']} å¤©")
        print(f"åˆ†ææºæ•°é‡: {report['analyzed_sources']} / {report['sources']}")
        print(f"å¹³å‡ç¼“å­˜åŠ é€Ÿæ¯”: {report['summary']['avg_speedup']:.2f}x")
        print(f"æœ‰æ•ˆç¼“å­˜æ¯”ä¾‹: {report['summary']['effective_cache_count']} / {report['analyzed_sources']} ({report['summary']['effective_cache_count']/report['analyzed_sources']*100:.1f}%)")
        print(f"å¹³å‡ç¼“å­˜å‘½ä¸­ç‡: {report['summary']['avg_cache_hit_rate']*100:.1f}%")
        
        # æ‰“å°è¡¨æ ¼
        table_data = []
        for detail in sorted(report['source_details'], key=lambda x: x['performance']['speedup'], reverse=True):
            perf = detail['performance']
            table_data.append([
                detail['source_id'],
                detail['name'],
                detail['type'],
                f"{perf['speedup']:.2f}x",
                "âœ“" if perf['cache_effective'] else "âœ—",
                f"{perf['cache_hit_rate']*100:.1f}%",
                f"{detail['cache_ttl']//60} åˆ†é’Ÿ",
                f"{detail['update_interval']//60} åˆ†é’Ÿ",
                f"{perf['avg_news_count']:.1f}"
            ])
        
        headers = ["æºID", "åç§°", "ç±»å‹", "åŠ é€Ÿæ¯”", "æœ‰æ•ˆ", "å‘½ä¸­ç‡", "ç¼“å­˜TTL", "æ›´æ–°é—´éš”", "å¹³å‡æ¡æ•°"]
        print("\n" + tabulate(table_data, headers=headers, tablefmt="grid"))
        
        # è¾“å‡ºå»ºè®®
        print("\n" + "-"*80)
        print("ç¼“å­˜ä¼˜åŒ–å»ºè®®:")
        print("-"*80)
        
        # æ‰¾å‡ºç¼“å­˜ä¸æœ‰æ•ˆçš„æº
        ineffective = [d for d in report['source_details'] if not d['performance']['cache_effective']]
        if ineffective:
            print(f"\nä»¥ä¸‹ {len(ineffective)} ä¸ªæºç¼“å­˜æ€§èƒ½ä¸ä½³ï¼Œéœ€è¦ä¼˜åŒ–:")
            for d in ineffective:
                print(f"  - {d['source_id']} ({d['name']}): åŠ é€Ÿæ¯” {d['performance']['speedup']:.2f}x")
            
        # æ‰¾å‡ºTTLè¿‡é•¿æˆ–è¿‡çŸ­çš„æº
        ttl_issues = [d for d in report['source_details'] 
                      if (d['cache_ttl'] > d['update_interval'] * 0.8) or 
                         (d['cache_ttl'] < d['update_interval'] * 0.3 and d['cache_ttl'] > 0)]
        if ttl_issues:
            print(f"\nä»¥ä¸‹ {len(ttl_issues)} ä¸ªæºç¼“å­˜TTLé…ç½®å¯èƒ½éœ€è¦è°ƒæ•´:")
            for d in ttl_issues:
                if d['cache_ttl'] > d['update_interval'] * 0.8:
                    print(f"  - {d['source_id']}: TTLè¿‡é•¿ ({d['cache_ttl']//60}åˆ†é’Ÿ vs æ›´æ–°é—´éš”{d['update_interval']//60}åˆ†é’Ÿ)")
                else:
                    print(f"  - {d['source_id']}: TTLè¿‡çŸ­ ({d['cache_ttl']//60}åˆ†é’Ÿ vs æ›´æ–°é—´éš”{d['update_interval']//60}åˆ†é’Ÿ)")
        
        print("\n" + "="*80)
            
    def plot_performance(self, report, output_file="cache_performance.png"):
        """ç»˜åˆ¶æ€§èƒ½å›¾è¡¨"""
        try:
            if not report or not report['source_details']:
                logger.error("æ— æŠ¥å‘Šæ•°æ®å¯ç»˜åˆ¶")
                return
                
            source_ids = [d['source_id'] for d in report['source_details']]
            speedups = [d['performance']['speedup'] for d in report['source_details']]
            hit_rates = [d['performance']['cache_hit_rate'] * 100 for d in report['source_details']]
            
            # æŒ‰åŠ é€Ÿæ¯”æ’åº
            sorted_indices = sorted(range(len(speedups)), key=lambda i: speedups[i], reverse=True)
            source_ids = [source_ids[i] for i in sorted_indices]
            speedups = [speedups[i] for i in sorted_indices]
            hit_rates = [hit_rates[i] for i in sorted_indices]
            
            # ç»˜å›¾
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
            
            # ç»˜åˆ¶åŠ é€Ÿæ¯”
            bars1 = ax1.bar(source_ids, speedups, color='skyblue')
            ax1.axhline(y=1.5, color='r', linestyle='-', alpha=0.7, label='æœ‰æ•ˆåŠ é€Ÿé˜ˆå€¼ (1.5x)')
            ax1.set_title('ç¼“å­˜åŠ é€Ÿæ¯” (è¶Šé«˜è¶Šå¥½)')
            ax1.set_xlabel('æ–°é—»æºID')
            ax1.set_ylabel('åŠ é€Ÿæ¯”')
            ax1.legend()
            
            # é«˜äº®æœ‰æ•ˆåŠ é€Ÿçš„æ¡
            for i, bar in enumerate(bars1):
                if speedups[i] >= 1.5:
                    bar.set_color('green')
                    
            # æ—‹è½¬xè½´æ ‡ç­¾ä»¥é¿å…é‡å 
            plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')
            
            # ç»˜åˆ¶å‘½ä¸­ç‡
            bars2 = ax2.bar(source_ids, hit_rates, color='lightgreen')
            ax2.axhline(y=90, color='r', linestyle='-', alpha=0.7, label='ç†æƒ³å‘½ä¸­ç‡ (90%)')
            ax2.set_title('ç¼“å­˜å‘½ä¸­ç‡ (è¶Šé«˜è¶Šå¥½)')
            ax2.set_xlabel('æ–°é—»æºID')
            ax2.set_ylabel('å‘½ä¸­ç‡ (%)')
            ax2.legend()
            
            # é«˜äº®é«˜å‘½ä¸­ç‡çš„æ¡
            for i, bar in enumerate(bars2):
                if hit_rates[i] >= 90:
                    bar.set_color('green')
                    
            # æ—‹è½¬xè½´æ ‡ç­¾ä»¥é¿å…é‡å 
            plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')
            
            # è°ƒæ•´å¸ƒå±€
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            plt.savefig(output_file)
            logger.info(f"æ€§èƒ½å›¾è¡¨å·²ä¿å­˜åˆ°: {output_file}")
            
            # å…³é—­å›¾è¡¨
            plt.close()
        except Exception as e:
            logger.error(f"ç»˜åˆ¶å›¾è¡¨æ—¶å‡ºé”™: {str(e)}")

async def register_sources():
    """æ³¨å†Œæºï¼Œä½¿ç”¨æ–°çš„ç¼“å­˜å¢å¼ºå™¨ç»Ÿä¸€æ”¹è¿›æ‰€æœ‰æº"""
    logging.info("åˆå§‹åŒ–æ–°é—»æä¾›è€…...")
    provider = DefaultNewsSourceProvider()
    
    # å¢å¼ºæ‰€æœ‰æº
    enhance_all_sources(provider)
    
    logging.info(f"æˆåŠŸæ³¨å†Œäº† {len(provider.get_all_sources())} ä¸ªæ–°é—»æºï¼Œæ‰€æœ‰æºå·²ç»è¿‡ç¼“å­˜å¢å¼º")
    return provider

async def test_source_caching(source_id: str, test_count: int = 3):
    """æµ‹è¯•æŒ‡å®šæºçš„ç¼“å­˜è¡Œä¸º"""
    logging.info(f"å¼€å§‹æµ‹è¯• {source_id} æºçš„ç¼“å­˜è¡Œä¸º...")
    
    provider = DefaultNewsSourceProvider()
    
    # è·å–æºå¹¶åº”ç”¨ç¼“å­˜å¢å¼º
    provider_sources = provider.get_all_sources()
    
    # æŸ¥æ‰¾æŒ‡å®šçš„æº
    source = None
    if isinstance(provider_sources, dict):
        if source_id not in provider_sources:
            logging.error(f"æœªæ‰¾åˆ°æº {source_id}")
            return
        source = provider_sources[source_id]
    else:
        # å¦‚æœprovider_sourcesæ˜¯åˆ—è¡¨ï¼Œéå†æŸ¥æ‰¾åŒ¹é…çš„source_id
        source = next((s for s in provider_sources if s.source_id == source_id), None)
        if not source:
            logging.error(f"æœªæ‰¾åˆ°æº {source_id}")
            return
    
    # æ³¨å†Œåˆ°ç¼“å­˜ç›‘æ§å™¨ - è¿™ä¼šè‡ªåŠ¨å¢å¼ºæº
    cache_monitor.register_source(source)
    
    logging.info(f"æºåˆå§‹åŒ–å®Œæˆ: {source.source_id} - {source.name}")
    logging.info(f"ç¼“å­˜è®¾ç½®: update_interval={source.update_interval}, cache_ttl={source.cache_ttl}")
    
    # æµ‹è¯•åœºæ™¯1: å¼ºåˆ¶æ›´æ–° (ç»•è¿‡ç¼“å­˜ï¼Œæµ‹è¯•å®é™…è·å–æ€§èƒ½)
    logging.info("\n=== åœºæ™¯1: å¼ºåˆ¶æ›´æ–° ===")
    forced_times = []
    for i in range(test_count):
        start_time = time.time()
        news = await source.get_news(force_update=True)
        elapsed = time.time() - start_time
        forced_times.append(elapsed)
        
        logging.info(f"å¼ºåˆ¶æ›´æ–° #{i+1}: è·å–äº† {len(news)} æ¡æ–°é—», è€—æ—¶: {elapsed:.3f}ç§’")
        if i < test_count - 1:
            await asyncio.sleep(1)  # çŸ­æš‚æš‚åœé˜²æ­¢è¯·æ±‚é¢‘ç‡è¿‡é«˜
    
    avg_forced = sum(forced_times) / len(forced_times)
    logging.info(f"å¼ºåˆ¶æ›´æ–°å¹³å‡è€—æ—¶: {avg_forced:.3f}ç§’")
    
    # æµ‹è¯•åœºæ™¯2: ç¼“å­˜æ£€ç´¢ (ä¸å¼ºåˆ¶æ›´æ–°ï¼Œä½¿ç”¨ç¼“å­˜)
    logging.info("\n=== åœºæ™¯2: ç¼“å­˜æ£€ç´¢ ===")
    cache_times = []
    for i in range(test_count):
        start_time = time.time()
        news = await source.get_news(force_update=False)
        elapsed = time.time() - start_time
        cache_times.append(elapsed)
        
        logging.info(f"ç¼“å­˜æ£€ç´¢ #{i+1}: è·å–äº† {len(news)} æ¡æ–°é—», è€—æ—¶: {elapsed:.3f}ç§’")
        if i < test_count - 1:
            await asyncio.sleep(0.5)
    
    avg_cache = sum(cache_times) / len(cache_times) if cache_times else 0
    logging.info(f"ç¼“å­˜æ£€ç´¢å¹³å‡è€—æ—¶: {avg_cache:.3f}ç§’")
    
    # æµ‹è¯•åœºæ™¯3: ç¼“å­˜æ¸…é™¤åæ£€ç´¢ (æ¨¡æ‹Ÿç¼“å­˜è¿‡æœŸ)
    logging.info("\n=== åœºæ™¯3: ç¼“å­˜æ¸…é™¤ ===")
    await source.clear_cache()
    start_time = time.time()
    news = await source.get_news()
    elapsed = time.time() - start_time
    
    logging.info(f"æ¸…é™¤ç¼“å­˜åæ£€ç´¢: è·å–äº† {len(news)} æ¡æ–°é—», è€—æ—¶: {elapsed:.3f}ç§’")
    
    # æ±‡æ€»å½“å‰ç¼“å­˜æŒ‡æ ‡å’Œä¿æŠ¤çŠ¶æ€
    cache_status = source.cache_status()
    
    # è®¡ç®—åŠ é€Ÿæ¯”
    acceleration_ratio = avg_forced / max(avg_cache, 0.001)
    
    logging.info("\n=== æµ‹è¯•ç»“æœæ±‡æ€» ===")
    logging.info(f"å¼ºåˆ¶æ›´æ–°å¹³å‡è€—æ—¶: {avg_forced:.3f}ç§’")
    logging.info(f"ç¼“å­˜æ£€ç´¢å¹³å‡è€—æ—¶: {avg_cache:.3f}ç§’")
    logging.info(f"ç¼“å­˜åŠ é€Ÿæ¯”: {acceleration_ratio:.2f}x")
    
    # è¾“å‡ºç¼“å­˜çŠ¶æ€å’Œä¿æŠ¤æŒ‡æ ‡
    logging.info("\n=== ç¼“å­˜çŠ¶æ€å’Œä¿æŠ¤æŒ‡æ ‡ ===")
    logging.info(f"ç¼“å­˜æ¡ç›®æ•°: {cache_status['cache_state']['items_count']}")
    logging.info(f"ç¼“å­˜æœ‰æ•ˆæ€§: {cache_status['cache_state']['valid']}")
    logging.info(f"ç¼“å­˜å‘½ä¸­ç‡: {cache_status['metrics']['hit_ratio']:.2%}")
    
    # ä¿æŠ¤ç»Ÿè®¡
    protections = cache_status['protection_stats']
    logging.info(f"ä¿æŠ¤ç»Ÿè®¡: ç©ºç»“æœä¿æŠ¤={protections['empty_protection_count']}, " +
                 f"é”™è¯¯ä¿æŠ¤={protections['error_protection_count']}, " +
                 f"æ•°æ®å‡å°‘ä¿æŠ¤={protections['shrink_protection_count']}")
    
    if protections['total_protection_count'] > 0:
        logging.info("ä¿æŠ¤æœºåˆ¶å·²æ¿€æ´»ï¼Œç¼“å­˜ä¿æŒäº†æ•°æ®å®Œæ•´æ€§")
    
    return True

async def enhanced_cache_status(source_id: Optional[str] = None):
    """æ˜¾ç¤ºæºçš„å¢å¼ºç¼“å­˜çŠ¶æ€"""
    provider = DefaultNewsSourceProvider()
    
    # å¢å¼ºæ‰€æœ‰æº
    enhance_all_sources(provider)
    
    if source_id:
        # æ˜¾ç¤ºå•ä¸ªæºçš„è¯¦ç»†çŠ¶æ€
        sources = provider.get_all_sources()
        
        # æŸ¥æ‰¾æŒ‡å®šçš„æº
        source = None
        if isinstance(sources, dict):
            if source_id not in sources:
                logging.error(f"æœªæ‰¾åˆ°æº {source_id}")
                return
            source = sources[source_id]
        else:
            # å¦‚æœsourcesæ˜¯åˆ—è¡¨ï¼Œéå†æŸ¥æ‰¾åŒ¹é…çš„source_id
            source = next((s for s in sources if s.source_id == source_id), None)
            if not source:
                logging.error(f"æœªæ‰¾åˆ°æº {source_id}")
                return
        
        status = source.cache_status()
        
        logging.info(f"\n=== {source_id} ({source.name}) ç¼“å­˜çŠ¶æ€ ===")
        logging.info(f"ç¼“å­˜é…ç½®: update_interval={status['cache_config']['update_interval']}ç§’, " +
                     f"cache_ttl={status['cache_config']['cache_ttl']}ç§’")
        
        # æ˜¾ç¤ºè‡ªé€‚åº”è®¾ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
        if status['cache_config']['adaptive_enabled']:
            logging.info(f"è‡ªé€‚åº”é—´éš”: {status['cache_config']['current_adaptive_interval']}ç§’")
        
        # ç¼“å­˜çŠ¶æ€
        cache_state = status['cache_state']
        logging.info("\nç¼“å­˜çŠ¶æ€:")
        logging.info(f"  æ¡ç›®æ•°: {cache_state['items_count']}")
        logging.info(f"  æœ€åæ›´æ–°: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(cache_state['last_update']))}")
        logging.info(f"  ç¼“å­˜å¹´é¾„: {cache_state['cache_age_seconds']:.1f}ç§’")
        logging.info(f"  æ˜¯å¦è¿‡æœŸ: {cache_state['is_expired']}")
        logging.info(f"  æ˜¯å¦æœ‰æ•ˆ: {cache_state['valid']}")
        
        # ä¿æŠ¤ç»Ÿè®¡
        protections = status['protection_stats']
        logging.info("\nä¿æŠ¤ç»Ÿè®¡:")
        logging.info(f"  ç©ºç»“æœä¿æŠ¤: {protections['empty_protection_count']} æ¬¡")
        logging.info(f"  é”™è¯¯ä¿æŠ¤: {protections['error_protection_count']} æ¬¡")
        logging.info(f"  æ•°æ®å‡å°‘ä¿æŠ¤: {protections['shrink_protection_count']} æ¬¡")
        logging.info(f"  æ€»ä¿æŠ¤æ¬¡æ•°: {protections['total_protection_count']} æ¬¡")
        
        if protections['recent_protections']:
            logging.info("\næœ€è¿‘ä¿æŠ¤äº‹ä»¶:")
            for event in protections['recent_protections']:
                event_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(event['time']))
                if event['type'] == 'empty_protection':
                    logging.info(f"  {event_time}: ç©ºç»“æœä¿æŠ¤ (ä¿æŠ¤äº† {event['cache_size']} æ¡æ•°æ®)")
                elif event['type'] == 'error_protection':
                    logging.info(f"  {event_time}: é”™è¯¯ä¿æŠ¤ - {event['error']} (ä¿æŠ¤äº† {event['cache_size']} æ¡æ•°æ®)")
                elif event['type'] == 'shrink_protection':
                    ratio = event['reduction_ratio'] * 100
                    logging.info(f"  {event_time}: æ•°æ®å‡å°‘ä¿æŠ¤ - ä» {event['old_size']} åˆ° {event['new_size']} æ¡ (å‡å°‘ {ratio:.1f}%)")
        
        # æ€§èƒ½æŒ‡æ ‡
        metrics = status['metrics']
        logging.info("\næ€§èƒ½æŒ‡æ ‡:")
        logging.info(f"  ç¼“å­˜å‘½ä¸­: {metrics['cache_hit_count']} æ¬¡")
        logging.info(f"  ç¼“å­˜æœªå‘½ä¸­: {metrics['cache_miss_count']} æ¬¡")
        logging.info(f"  å‘½ä¸­ç‡: {metrics['hit_ratio']:.2%}")
        logging.info(f"  ç©ºç»“æœæ¬¡æ•°: {metrics['empty_result_count']} æ¬¡")
        logging.info(f"  è·å–é”™è¯¯æ¬¡æ•°: {metrics['fetch_error_count']} æ¬¡")
    else:
        # æ˜¾ç¤ºå…¨å±€çŠ¶æ€
        global_status = cache_monitor.get_global_status()
        metrics = global_status['global_metrics']
        
        logging.info("\n=== å…¨å±€ç¼“å­˜çŠ¶æ€ ===")
        logging.info(f"å¢å¼ºçš„æºæ•°é‡: {len(global_status['sources'])}")
        logging.info(f"æ€»ç¼“å­˜å‘½ä¸­: {metrics['total_cache_hits']} æ¬¡")
        logging.info(f"æ€»ç¼“å­˜æœªå‘½ä¸­: {metrics['total_cache_misses']} æ¬¡")
        logging.info(f"å…¨å±€å‘½ä¸­ç‡: {metrics['global_hit_ratio']:.2%}")
        logging.info(f"æ€»ä¿æŠ¤æ¬¡æ•°: {metrics['total_protections']} æ¬¡")
        
        # ä¿æŠ¤åˆ†ç±»
        logging.info("\nä¿æŠ¤åˆ†ç±»:")
        logging.info(f"  ç©ºç»“æœä¿æŠ¤: {metrics['protection_breakdown']['empty_protections']} æ¬¡")
        logging.info(f"  é”™è¯¯ä¿æŠ¤: {metrics['protection_breakdown']['error_protections']} æ¬¡")
        logging.info(f"  æ•°æ®å‡å°‘ä¿æŠ¤: {metrics['protection_breakdown']['shrink_protections']} æ¬¡")
        
        # æºçŠ¶æ€æ‘˜è¦
        logging.info("\næºæ‘˜è¦:")
        for source_id, source_status in global_status['sources'].items():
            protection_count = source_status['protection_stats']['total_protection_count']
            cache_size = source_status['cache_state']['items_count']
            hit_ratio = source_status['metrics']['hit_ratio']
            
            protection_indicator = "ğŸ›¡ï¸ " if protection_count > 0 else ""
            logging.info(f"  {protection_indicator}{source_id}: {cache_size}æ¡æ•°æ®, å‘½ä¸­ç‡={hit_ratio:.2%}, ä¿æŠ¤æ¬¡æ•°={protection_count}")

async def main():
    parser = argparse.ArgumentParser(description="æ–°é—»æºç¼“å­˜ç›‘æ§å·¥å…·")
    parser.add_argument("--register", action="store_true", help="æ³¨å†Œæ‰€æœ‰æ–°é—»æº")
    parser.add_argument("--test", type=str, help="æµ‹è¯•æŒ‡å®šæºçš„ç¼“å­˜è¡Œä¸º")
    parser.add_argument("--status", action="store_true", help="æ˜¾ç¤ºæ‰€æœ‰æºçš„ç¼“å­˜çŠ¶æ€")
    parser.add_argument("--source-status", type=str, help="æ˜¾ç¤ºæŒ‡å®šæºçš„è¯¦ç»†ç¼“å­˜çŠ¶æ€")
    parser.add_argument("--debug", action="store_true", help="å¼€å¯è°ƒè¯•æ—¥å¿—")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    log_level = logging.DEBUG if args.debug else logging.INFO
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[logging.StreamHandler()]
    )
    
    if args.register:
        await register_sources()
    elif args.test:
        await test_source_caching(args.test)
    elif args.status:
        await enhanced_cache_status()
    elif args.source_status:
        await enhanced_cache_status(args.source_status)
    else:
        parser.print_help()

if __name__ == "__main__":
    asyncio.run(main()) 